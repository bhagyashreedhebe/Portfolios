COMP6200 Data Science Portfolio 
===

Portfolio 1:

1. Analysis of CSV data for cycling strava and cheetah. Used inner join to join the two dataframes for which output will check the data on the index values. For data in strava if it is found in cheetah then the resultant dataframe will contain such entries. So the goal to keep only those rows of data that appear in both data frames so that we have complete data for every row is being statisfied by this type of join.
2. Since the date was being placed in the index 
3. Task 1 was to remove rides with no measured power (where device_watts is False). Checked for the unique values for device_watts field and found that there were Nan values also. Either the dataframe can be filtered by selecting just the true values or drop the rides with Nan values and select the filter out  false fields.
4. Task 2 was to look at the distributions of some key variables: time, distance, average speed, average power, TSS. Are they normally distributed? Skewed? I have used different ways to answer this question. First of all used subplot to plot two graphs of distance v/s TSS and distance v/s average_watts at a time. There appeared to be a linear relationship in both the plots. Used histogram to plot the time in seconds  for which there there was a right skewed histogram depicted. Created a dataframe indexes which contained the filtered out the joined_df for our varibales of interest.Skewness is the measure of asymmetry of the distribution. Applied skew() function to it. Now skew() is an in-built function we get the skewness of the data over a requested axis wherein a value closer to 0 represents a normal distribution.
5. Task 3 was to explore the relationships between the following variables. Are any of them corrolated with each other (do they vary together in a predictable way)? Can you explain any relationships you observe? For this purpose I have used a heatmap.  Heatmap is a rectangular color encoded matrix which shows the correlation between variables on a scale of 1 to -1. Correlation between the variables of our interest in indexes dataframe was found out. This correlation dataframe was used to generate the heatmap.
6. Task 4 was to find out if Races are more challenging than Rides in general? Firstly grouped the data on the 'workout_type' and applied median() to it, here we can compare the median NP values for different workout types. It is evident that Race is the highest as compared to others. Then splitted the joined_df for workout_type 'Race' and others two different dataframes. Heartrate is parameter which is very much related to a workout type hence was considered for comparision. The plot for the same reflected the assumption by having higher value for Average Heartrate in Races.
7. Task 5 was to generate a plot that summarises the number of km ridden each month over the period of the data. Made use of differnt plotting techniques for data visualisation. From the plot of month_df it looks like that the lowest activity was between May-2018 to September-2018.
8. This portofio was more about data visualisation. Data depending upon different features was to be represented to audience.

Portfolio 2:

Analysing COVID-19 Data
1. Data for Covid-19 available by Johns Hopkins University in "https://github.com/CSSEGISandData/COVID-19" GitHub repository.
2. The data has country and region names and not just countries. Made us of groupby to group the countries depending on the country name and dropped irrelevant coulmns of Latitude and Longitude.
3. loc() method is used to filter and select data based on labels and our labels are 'Country/Region' and dates.
4. To compare the data for different countries we should plot for different countries. So I have selected the data using loc on the index which is or 'Country/Region'.
5. The challenge given to us was to create a visualisation showing the data for different countries aligned from the time that they have 100 confirmed cases. For this purpose I used a copy of grouped dataframe and filtered out the values that are above 100 or else have set them to NaN. Extracted the data for countries of our interest in my_countries df. Saved it by taking the transpose (swapping rows and columns). Dropped the NaN values and saved into my_new_countries df and created plot for it. Using ggplot for better visualization, it changes the label color to grey and axis background to light grey.  We can see the trend of growths observed depending upon the countries in the plots.
6. Normalisation by Population : Population data was downloaded from https://datahub.io/JohnSnowLabs/population-figures-by-country#data for which the latest entry was of year 2016. Considered this entry for our analysis in country_population dataframe. Created latest_df to contain the data for the latest date as Cases. Merged the dataframe with the country_population using merge() by giving columns in the function and created dfinal df. Added a column for "per million cases" to document the number of cases per million of the population. Plotted these values in a bar plot which showed maximum number of cases per million in US.
7. Used two linear regression modeling by considering the countries US and China. Results were found out to be better for model considering US as y variable.

Portfolio 3:

The aim of this portfolio build a model to predict the genre of books for a given dataset. We consider five genre only in our analysis. The model was built upon the summary of the books. I have used TFIDF algorithm and applied Logistic Regression to it. Text ceaning function is used to clean the data for summary i.e. remove backslashes and apostrophes and saved as clean_summary. nltk package stopwords were used to remove the common stopwords defined for english language in the package and the clean_summary was modified. sklearn has a feature Labelencoder which encodes our target variable to 0 and number of classes-1. As there are five classes or genre in our data they were encoded into 0 to 4 values. Now we need to extract the features using clean_summary and TFIDFVectorizer was used for this. TFIDF will get the term frequency inverse document frequency Here I have considered 10000 features i.e. the most frequent words. Then the clean_summary was split into test and train datasets with a test size of 25% of the original dataset. TFIDF features (X_train_tfidf, X_test_tfidf) were created for train and test set. As we have five classes(genres) I have used Logistic regression to build the model. OneVsRestClassifier is used for multiclass classification. The model was fit on the training data and predications were made on test. inverse_transform() function convert the predicted array into the genre. 
